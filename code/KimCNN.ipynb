{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5187b2-cbf1-4578-8ddf-0fd3c266cca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from load_data import load_dataset\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43804133-1cf2-44a1-b10a-2e65c5d848ab",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc7715d-53b9-4561-acd7-84b9f9d5e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KimCNN Model Structure\n",
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, mode, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super(KimCNN, self).__init__()\n",
    "        output_channel = n_filters\n",
    "        target_class = output_dim\n",
    "        words_num = vocab_size\n",
    "        words_dim = embedding_dim\n",
    "        embed_num = vocab_size\n",
    "        embed_dim = embedding_dim\n",
    "        self.mode = mode\n",
    "        Ks = 3 # There are three conv net here\n",
    "        if self.mode == 'multichannel':\n",
    "            input_channel = 2\n",
    "        else:\n",
    "            input_channel = 1\n",
    "        self.embed = nn.Embedding(words_num, words_dim)\n",
    "        self.static_embed = nn.Embedding(embed_num, embed_dim)\n",
    "        self.non_static_embed = nn.Embedding(embed_num, embed_dim)\n",
    "        self.static_embed.weight.requires_grad = False\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channel, output_channel, (3, words_dim), padding=(2,0))\n",
    "        self.conv2 = nn.Conv2d(input_channel, output_channel, (4, words_dim), padding=(3,0))\n",
    "        self.conv3 = nn.Conv2d(input_channel, output_channel, (5, words_dim), padding=(4,0))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(Ks * output_channel, target_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'rand':\n",
    "            word_input = self.embed(x) # (batch, sent_len, embed_dim)\n",
    "            x = word_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n",
    "        elif self.mode == 'static':\n",
    "            static_input = self.static_embed(x)\n",
    "            x = static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n",
    "        elif self.mode == 'non-static':\n",
    "            non_static_input = self.non_static_embed(x)\n",
    "            x = non_static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n",
    "        elif self.mode == 'multichannel':\n",
    "            non_static_input = self.non_static_embed(x)\n",
    "            static_input = self.static_embed(x)\n",
    "            x = torch.stack([non_static_input, static_input], dim=1) # (batch, channel_input=2, sent_len, embed_dim)\n",
    "        else:\n",
    "            print(\"Unsupported Mode\")\n",
    "            exit()\n",
    "        x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n",
    "        # (batch, channel_output, ~=sent_len) * Ks\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # max-over-time pooling\n",
    "        # (batch, channel_output) * Ks\n",
    "        x = torch.cat(x, 1) # (batch, channel_output * Ks)\n",
    "        x = self.dropout(x)\n",
    "        logit = self.fc1(x) # (batch, target_size)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c791bd-1dd3-4e22-ae32-61ad634b8ed5",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a71bd3-a589-4fd0-b20a-b8cf886c4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def multi_models(model, train_iterator, valid_iterator,  test_iterator, num = 5, N_EPOCHS = 20, early_stopping = 5):\n",
    "    init_model = copy.deepcopy(model)\n",
    "    res = {}\n",
    "    res[\"test_loss\"] = []\n",
    "    res[\"test_acc\"] = []\n",
    "    ### begin training process\n",
    "    for i in range(num):\n",
    "        print('Begin training model %s'%i)\n",
    "        res[\"model_%s\"%i] = {}\n",
    "        res[\"model_%s\"%i][\"time\"] = []\n",
    "        res[\"model_%s\"%i][\"train_loss\"] = []\n",
    "        res[\"model_%s\"%i][\"val_loss\"] = []\n",
    "        res[\"model_%s\"%i][\"train_acc\"] = []\n",
    "        res[\"model_%s\"%i][\"val_acc\"] = []\n",
    "        best_valid_loss = float('inf')\n",
    "        num_steps = 0\n",
    "        ### reset model for each round\n",
    "        # model.load_state_dict(init_param)\n",
    "        model = copy.deepcopy(init_model)\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion = criterion.to(device)\n",
    "        \n",
    "        for epoch in range(N_EPOCHS):\n",
    "            print('Begin epoch %s'%epoch)\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "            valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "            end_time = time.time()\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), 'best-model.pt')\n",
    "                num_steps = 0\n",
    "            else:\n",
    "                num_steps += 1\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "            res[\"model_%s\"%i][\"time\"].append(end_time - start_time)\n",
    "            res[\"model_%s\"%i][\"train_loss\"].append(train_loss)\n",
    "            res[\"model_%s\"%i][\"val_loss\"].append(valid_loss)\n",
    "            res[\"model_%s\"%i][\"train_acc\"].append(train_acc)\n",
    "            res[\"model_%s\"%i][\"val_acc\"].append(valid_acc)\n",
    "            if num_steps >= early_stopping:\n",
    "                break\n",
    "        \n",
    "        model.load_state_dict(torch.load('best-model.pt'))\n",
    "        test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        res[\"test_loss\"].append(test_loss)\n",
    "        res[\"test_acc\"].append(test_acc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff42d1-398a-4fc6-8f9e-e203ba3d01e5",
   "metadata": {},
   "source": [
    "### Train model on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344936a2-775c-4a5c-9ab8-c134b326b5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"IMDB\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56aeb7f-4c5a-4a15-8e18-b696786d499b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Statis Model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "MODE = \"static\"\n",
    "\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_static_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"non-static\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_nonstatic_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"multichannel\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_multichannel_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987462b6-3e47-4156-a36c-18f62f2a7a0a",
   "metadata": {},
   "source": [
    "### Train model on TREC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913eaa9c-e767-4da8-9848-e9ce0acd1e11",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 5.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 1.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.048 | Train Acc: 62.69%\n",
      "\t Val. Loss: 0.595 |  Val. Acc: 81.81%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.514 | Train Acc: 82.71%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 85.46%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.353 | Train Acc: 88.57%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 87.19%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.260 | Train Acc: 91.65%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 87.19%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.195 | Train Acc: 94.28%\n",
      "\t Val. Loss: 0.353 |  Val. Acc: 87.37%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.147 | Train Acc: 95.95%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 89.43%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.106 | Train Acc: 97.40%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 88.06%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.90%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 89.44%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.073 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 88.58%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.061 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.331 |  Val. Acc: 88.07%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.048 | Train Acc: 99.10%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 88.07%\n",
      "Test Loss: 0.249 | Test Acc: 90.44%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.053 | Train Acc: 61.80%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 80.63%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.509 | Train Acc: 82.79%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 86.33%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.349 | Train Acc: 88.54%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 87.01%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.250 | Train Acc: 92.32%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 87.72%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.186 | Train Acc: 94.40%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 89.09%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.142 | Train Acc: 96.20%\n",
      "\t Val. Loss: 0.315 |  Val. Acc: 88.77%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.106 | Train Acc: 97.27%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 88.06%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.091 | Train Acc: 97.67%\n",
      "\t Val. Loss: 0.313 |  Val. Acc: 88.42%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.069 | Train Acc: 98.62%\n",
      "\t Val. Loss: 0.306 |  Val. Acc: 89.29%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.056 | Train Acc: 98.88%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 89.10%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.044 | Train Acc: 99.20%\n",
      "\t Val. Loss: 0.331 |  Val. Acc: 88.41%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.038 | Train Acc: 99.32%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 88.77%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.035 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 88.94%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.028 | Train Acc: 99.55%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 88.59%\n",
      "Test Loss: 0.273 | Test Acc: 90.40%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.069 | Train Acc: 61.98%\n",
      "\t Val. Loss: 0.592 |  Val. Acc: 81.64%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.514 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.442 |  Val. Acc: 84.76%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.356 | Train Acc: 88.21%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 87.54%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.256 | Train Acc: 91.99%\n",
      "\t Val. Loss: 0.341 |  Val. Acc: 88.06%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.192 | Train Acc: 94.35%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 87.21%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.145 | Train Acc: 95.91%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 87.38%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.107 | Train Acc: 97.40%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 88.92%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.086 | Train Acc: 97.88%\n",
      "\t Val. Loss: 0.318 |  Val. Acc: 89.79%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.068 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 88.25%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.059 | Train Acc: 98.87%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 89.28%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.90%\n",
      "\t Val. Loss: 0.334 |  Val. Acc: 88.58%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.85%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 88.92%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.041 | Train Acc: 99.22%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 88.58%\n",
      "Test Loss: 0.231 | Test Acc: 91.18%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.068 | Train Acc: 61.42%\n",
      "\t Val. Loss: 0.614 |  Val. Acc: 80.61%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.513 | Train Acc: 83.12%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 85.11%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.350 | Train Acc: 88.85%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 85.99%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.253 | Train Acc: 91.91%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 88.06%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.198 | Train Acc: 94.00%\n",
      "\t Val. Loss: 0.325 |  Val. Acc: 88.06%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.143 | Train Acc: 96.06%\n",
      "\t Val. Loss: 0.330 |  Val. Acc: 87.72%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.110 | Train Acc: 97.15%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 88.41%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.097 | Train Acc: 97.41%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 88.05%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.075 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 88.92%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.68%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 88.75%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.050 | Train Acc: 99.02%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 88.41%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.040 | Train Acc: 99.15%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 88.93%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.041 | Train Acc: 99.15%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 89.10%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.57%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 88.92%\n",
      "Test Loss: 0.237 | Test Acc: 91.38%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.052 | Train Acc: 61.69%\n",
      "\t Val. Loss: 0.595 |  Val. Acc: 81.30%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.511 | Train Acc: 82.76%\n",
      "\t Val. Loss: 0.425 |  Val. Acc: 85.46%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.348 | Train Acc: 88.35%\n",
      "\t Val. Loss: 0.381 |  Val. Acc: 85.99%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.250 | Train Acc: 92.13%\n",
      "\t Val. Loss: 0.347 |  Val. Acc: 87.53%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.190 | Train Acc: 94.17%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 88.05%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.137 | Train Acc: 96.44%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 89.09%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.109 | Train Acc: 97.12%\n",
      "\t Val. Loss: 0.332 |  Val. Acc: 87.36%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.083 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 88.24%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.069 | Train Acc: 98.58%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 88.40%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.68%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 88.92%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.96%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 89.27%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.038 | Train Acc: 99.28%\n",
      "\t Val. Loss: 0.337 |  Val. Acc: 89.60%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.57%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 88.41%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.53%\n",
      "\t Val. Loss: 0.379 |  Val. Acc: 88.25%\n",
      "Test Loss: 0.247 | Test Acc: 90.99%\n",
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.095 | Train Acc: 57.13%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 72.17%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.593 | Train Acc: 78.62%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 78.05%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.376 | Train Acc: 87.99%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 81.33%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.265 | Train Acc: 91.94%\n",
      "\t Val. Loss: 0.470 |  Val. Acc: 82.03%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.180 | Train Acc: 94.74%\n",
      "\t Val. Loss: 0.451 |  Val. Acc: 82.40%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.124 | Train Acc: 96.56%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 81.36%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.091 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 82.22%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.064 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.493 |  Val. Acc: 82.40%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.053 | Train Acc: 98.53%\n",
      "\t Val. Loss: 0.530 |  Val. Acc: 82.05%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.039 | Train Acc: 99.14%\n",
      "\t Val. Loss: 0.470 |  Val. Acc: 83.24%\n",
      "Test Loss: 0.372 | Test Acc: 86.36%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.099 | Train Acc: 57.22%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 71.50%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.588 | Train Acc: 78.81%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 78.94%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.395 | Train Acc: 87.02%\n",
      "\t Val. Loss: 0.562 |  Val. Acc: 78.22%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.269 | Train Acc: 91.71%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 81.00%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.178 | Train Acc: 95.03%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 80.32%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.122 | Train Acc: 96.85%\n",
      "\t Val. Loss: 0.475 |  Val. Acc: 81.71%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.86%\n",
      "\t Val. Loss: 0.486 |  Val. Acc: 82.72%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.060 | Train Acc: 98.54%\n",
      "\t Val. Loss: 0.526 |  Val. Acc: 81.34%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.59%\n",
      "\t Val. Loss: 0.498 |  Val. Acc: 82.40%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.82%\n",
      "\t Val. Loss: 0.521 |  Val. Acc: 82.92%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.037 | Train Acc: 99.23%\n",
      "\t Val. Loss: 0.581 |  Val. Acc: 81.18%\n",
      "Test Loss: 0.351 | Test Acc: 87.14%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.076 | Train Acc: 58.10%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 74.59%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.571 | Train Acc: 79.41%\n",
      "\t Val. Loss: 0.576 |  Val. Acc: 78.40%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.384 | Train Acc: 87.59%\n",
      "\t Val. Loss: 0.554 |  Val. Acc: 79.10%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.246 | Train Acc: 92.20%\n",
      "\t Val. Loss: 0.479 |  Val. Acc: 82.04%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.162 | Train Acc: 95.25%\n",
      "\t Val. Loss: 0.512 |  Val. Acc: 80.31%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.117 | Train Acc: 96.59%\n",
      "\t Val. Loss: 0.463 |  Val. Acc: 82.73%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.094 | Train Acc: 97.33%\n",
      "\t Val. Loss: 0.437 |  Val. Acc: 83.95%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.072 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.457 |  Val. Acc: 82.91%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.90%\n",
      "\t Val. Loss: 0.459 |  Val. Acc: 83.07%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.041 | Train Acc: 99.06%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 82.40%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.034 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.532 |  Val. Acc: 82.56%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.31%\n",
      "\t Val. Loss: 0.492 |  Val. Acc: 82.56%\n",
      "Test Loss: 0.377 | Test Acc: 84.71%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.098 | Train Acc: 57.49%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 71.11%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.600 | Train Acc: 78.88%\n",
      "\t Val. Loss: 0.579 |  Val. Acc: 78.40%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.392 | Train Acc: 87.13%\n",
      "\t Val. Loss: 0.544 |  Val. Acc: 78.26%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.271 | Train Acc: 91.08%\n",
      "\t Val. Loss: 0.477 |  Val. Acc: 81.53%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.168 | Train Acc: 95.10%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 82.40%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.123 | Train Acc: 96.65%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 81.00%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.089 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.517 |  Val. Acc: 81.88%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.067 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 82.39%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.056 | Train Acc: 98.54%\n",
      "\t Val. Loss: 0.501 |  Val. Acc: 82.71%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.045 | Train Acc: 99.00%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 82.74%\n",
      "Test Loss: 0.386 | Test Acc: 85.68%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.067 | Train Acc: 58.92%\n",
      "\t Val. Loss: 0.728 |  Val. Acc: 70.82%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.572 | Train Acc: 79.66%\n",
      "\t Val. Loss: 0.586 |  Val. Acc: 78.37%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.378 | Train Acc: 87.65%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 81.17%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.260 | Train Acc: 92.06%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 80.64%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.174 | Train Acc: 94.83%\n",
      "\t Val. Loss: 0.478 |  Val. Acc: 81.34%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.125 | Train Acc: 96.32%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 81.01%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.083 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 81.39%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.66%\n",
      "\t Val. Loss: 0.475 |  Val. Acc: 82.06%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.051 | Train Acc: 98.63%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 81.89%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.84%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 82.42%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.035 | Train Acc: 99.17%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 82.55%\n",
      "Test Loss: 0.394 | Test Acc: 84.66%\n",
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.945 | Train Acc: 63.84%\n",
      "\t Val. Loss: 0.582 |  Val. Acc: 78.06%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.435 | Train Acc: 85.44%\n",
      "\t Val. Loss: 0.445 |  Val. Acc: 82.91%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.251 | Train Acc: 92.21%\n",
      "\t Val. Loss: 0.391 |  Val. Acc: 84.97%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.161 | Train Acc: 95.24%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 86.35%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.103 | Train Acc: 97.05%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 85.46%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.068 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 86.34%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.62%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 86.70%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.037 | Train Acc: 99.17%\n",
      "\t Val. Loss: 0.376 |  Val. Acc: 87.19%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.035 | Train Acc: 99.13%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 87.37%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.18%\n",
      "\t Val. Loss: 0.398 |  Val. Acc: 86.68%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.028 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 86.53%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.46%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 86.00%\n",
      "Test Loss: 0.248 | Test Acc: 90.25%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.911 | Train Acc: 65.54%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 82.19%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.413 | Train Acc: 85.75%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 84.45%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.254 | Train Acc: 91.84%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 86.51%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.156 | Train Acc: 95.46%\n",
      "\t Val. Loss: 0.347 |  Val. Acc: 88.42%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.097 | Train Acc: 97.29%\n",
      "\t Val. Loss: 0.383 |  Val. Acc: 84.96%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.065 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 87.72%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.82%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 88.06%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.034 | Train Acc: 99.26%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 88.43%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.15%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 87.03%\n",
      "Test Loss: 0.281 | Test Acc: 89.62%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.921 | Train Acc: 65.40%\n",
      "\t Val. Loss: 0.552 |  Val. Acc: 79.10%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.432 | Train Acc: 85.14%\n",
      "\t Val. Loss: 0.425 |  Val. Acc: 85.47%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.249 | Train Acc: 92.50%\n",
      "\t Val. Loss: 0.382 |  Val. Acc: 86.86%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.161 | Train Acc: 95.41%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 86.52%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.102 | Train Acc: 96.83%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 86.35%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.069 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 86.86%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.83%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 87.73%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.039 | Train Acc: 99.09%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 87.21%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.034 | Train Acc: 99.15%\n",
      "\t Val. Loss: 0.389 |  Val. Acc: 87.19%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 88.23%\n",
      "Test Loss: 0.261 | Test Acc: 89.81%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.927 | Train Acc: 65.00%\n",
      "\t Val. Loss: 0.550 |  Val. Acc: 79.99%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.427 | Train Acc: 85.45%\n",
      "\t Val. Loss: 0.446 |  Val. Acc: 84.29%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.251 | Train Acc: 91.90%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 84.97%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.159 | Train Acc: 95.20%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 87.19%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.101 | Train Acc: 97.19%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 86.68%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.071 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 85.49%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.051 | Train Acc: 98.74%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 87.04%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.038 | Train Acc: 99.28%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 86.53%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 85.68%\n",
      "Test Loss: 0.286 | Test Acc: 89.03%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.937 | Train Acc: 63.35%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 80.99%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.414 | Train Acc: 86.23%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 83.42%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.250 | Train Acc: 92.18%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 86.00%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.157 | Train Acc: 95.43%\n",
      "\t Val. Loss: 0.375 |  Val. Acc: 86.00%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.87%\n",
      "\t Val. Loss: 0.350 |  Val. Acc: 86.18%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.078 | Train Acc: 97.89%\n",
      "\t Val. Loss: 0.407 |  Val. Acc: 86.71%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.052 | Train Acc: 99.05%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 87.74%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.036 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.374 |  Val. Acc: 87.39%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.37%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 87.23%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.026 | Train Acc: 99.52%\n",
      "\t Val. Loss: 0.387 |  Val. Acc: 87.22%\n",
      "Test Loss: 0.275 | Test Acc: 90.79%\n"
     ]
    }
   ],
   "source": [
    "data_name = \"TREC\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device)\n",
    "\n",
    "### Statis Model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "MODE = \"static\"\n",
    "\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_static_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"non-static\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_nonstatic_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"multichannel\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_multichannel_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677a7f9-e303-4b72-9a50-32f77c236265",
   "metadata": {},
   "source": [
    "### Train model on SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e1399e6-41e1-4fc8-b2d9-c1df5750965f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading trainDevTestTrees_PTB.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:00<00:00, 1.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n",
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.283 | Train Acc: 88.47%\n",
      "\t Val. Loss: 0.408 |  Val. Acc: 81.14%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.215 | Train Acc: 91.76%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 81.90%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.184 | Train Acc: 93.07%\n",
      "\t Val. Loss: 0.476 |  Val. Acc: 81.34%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.162 | Train Acc: 94.00%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 81.29%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.150 | Train Acc: 94.55%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 81.72%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.140 | Train Acc: 95.03%\n",
      "\t Val. Loss: 0.480 |  Val. Acc: 82.61%\n",
      "Test Loss: 0.385 | Test Acc: 82.62%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.286 | Train Acc: 88.31%\n",
      "\t Val. Loss: 0.407 |  Val. Acc: 81.85%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.214 | Train Acc: 91.73%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 83.91%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.183 | Train Acc: 93.10%\n",
      "\t Val. Loss: 0.453 |  Val. Acc: 80.78%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.162 | Train Acc: 94.07%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 82.79%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.148 | Train Acc: 94.66%\n",
      "\t Val. Loss: 0.451 |  Val. Acc: 83.57%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.138 | Train Acc: 95.08%\n",
      "\t Val. Loss: 0.467 |  Val. Acc: 82.99%\n",
      "Test Loss: 0.377 | Test Acc: 83.42%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.32%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 81.90%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.216 | Train Acc: 91.64%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 82.97%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.183 | Train Acc: 93.20%\n",
      "\t Val. Loss: 0.426 |  Val. Acc: 82.46%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.164 | Train Acc: 94.01%\n",
      "\t Val. Loss: 0.448 |  Val. Acc: 82.19%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.151 | Train Acc: 94.50%\n",
      "\t Val. Loss: 0.478 |  Val. Acc: 82.97%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.139 | Train Acc: 95.05%\n",
      "\t Val. Loss: 0.471 |  Val. Acc: 82.21%\n",
      "Test Loss: 0.387 | Test Acc: 82.60%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.36%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 83.26%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.215 | Train Acc: 91.70%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 82.30%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.183 | Train Acc: 93.18%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 83.35%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.165 | Train Acc: 93.98%\n",
      "\t Val. Loss: 0.466 |  Val. Acc: 81.79%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.150 | Train Acc: 94.56%\n",
      "\t Val. Loss: 0.471 |  Val. Acc: 82.01%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.137 | Train Acc: 95.09%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 83.75%\n",
      "Test Loss: 0.382 | Test Acc: 83.05%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.36%\n",
      "\t Val. Loss: 0.418 |  Val. Acc: 81.34%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.217 | Train Acc: 91.61%\n",
      "\t Val. Loss: 0.437 |  Val. Acc: 82.01%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.184 | Train Acc: 93.07%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 81.09%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.162 | Train Acc: 93.98%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 81.74%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.62%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 81.52%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.138 | Train Acc: 95.09%\n",
      "\t Val. Loss: 0.498 |  Val. Acc: 82.99%\n",
      "Test Loss: 0.395 | Test Acc: 82.29%\n",
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.417 | Train Acc: 80.67%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 77.17%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.47%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 74.46%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.166 | Train Acc: 94.05%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 77.63%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.136 | Train Acc: 95.20%\n",
      "\t Val. Loss: 0.857 |  Val. Acc: 76.32%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.86%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 79.71%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.47%\n",
      "\t Val. Loss: 0.880 |  Val. Acc: 77.46%\n",
      "Test Loss: 0.528 | Test Acc: 76.10%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.417 | Train Acc: 80.73%\n",
      "\t Val. Loss: 0.614 |  Val. Acc: 73.57%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.53%\n",
      "\t Val. Loss: 0.473 |  Val. Acc: 80.38%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.164 | Train Acc: 93.96%\n",
      "\t Val. Loss: 0.615 |  Val. Acc: 78.71%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.136 | Train Acc: 95.16%\n",
      "\t Val. Loss: 0.502 |  Val. Acc: 82.97%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.118 | Train Acc: 95.88%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 80.89%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.41%\n",
      "\t Val. Loss: 0.558 |  Val. Acc: 82.17%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.095 | Train Acc: 96.85%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 80.31%\n",
      "Test Loss: 0.504 | Test Acc: 78.65%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.418 | Train Acc: 80.50%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 77.48%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.223 | Train Acc: 91.60%\n",
      "\t Val. Loss: 0.487 |  Val. Acc: 80.49%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.167 | Train Acc: 93.98%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 79.31%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.134 | Train Acc: 95.26%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 79.26%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.117 | Train Acc: 95.93%\n",
      "\t Val. Loss: 0.657 |  Val. Acc: 80.04%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.104 | Train Acc: 96.45%\n",
      "\t Val. Loss: 0.813 |  Val. Acc: 78.46%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.095 | Train Acc: 96.82%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 80.98%\n",
      "Test Loss: 0.518 | Test Acc: 78.34%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.420 | Train Acc: 80.52%\n",
      "\t Val. Loss: 0.534 |  Val. Acc: 75.38%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.52%\n",
      "\t Val. Loss: 0.523 |  Val. Acc: 78.62%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.166 | Train Acc: 93.89%\n",
      "\t Val. Loss: 0.676 |  Val. Acc: 76.67%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.135 | Train Acc: 95.25%\n",
      "\t Val. Loss: 0.619 |  Val. Acc: 79.04%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.118 | Train Acc: 95.91%\n",
      "\t Val. Loss: 0.599 |  Val. Acc: 81.29%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.38%\n",
      "\t Val. Loss: 0.714 |  Val. Acc: 79.89%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.78%\n",
      "\t Val. Loss: 1.179 |  Val. Acc: 75.27%\n",
      "Test Loss: 0.521 | Test Acc: 79.20%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.417 | Train Acc: 80.74%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 75.60%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.49%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 81.29%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.165 | Train Acc: 94.04%\n",
      "\t Val. Loss: 0.633 |  Val. Acc: 77.66%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.137 | Train Acc: 95.22%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 80.60%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.83%\n",
      "\t Val. Loss: 0.588 |  Val. Acc: 81.45%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.102 | Train Acc: 96.42%\n",
      "\t Val. Loss: 1.023 |  Val. Acc: 77.19%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.094 | Train Acc: 96.84%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 79.89%\n",
      "Test Loss: 0.482 | Test Acc: 79.64%\n",
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.286 | Train Acc: 88.27%\n",
      "\t Val. Loss: 0.456 |  Val. Acc: 82.01%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.182 | Train Acc: 93.31%\n",
      "\t Val. Loss: 0.482 |  Val. Acc: 82.28%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.141 | Train Acc: 94.92%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 78.44%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.82%\n",
      "\t Val. Loss: 0.565 |  Val. Acc: 82.05%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.48%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 80.56%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.092 | Train Acc: 96.84%\n",
      "\t Val. Loss: 0.982 |  Val. Acc: 77.43%\n",
      "Test Loss: 0.403 | Test Acc: 82.80%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.287 | Train Acc: 88.23%\n",
      "\t Val. Loss: 0.488 |  Val. Acc: 79.98%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.181 | Train Acc: 93.34%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 81.07%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.142 | Train Acc: 94.92%\n",
      "\t Val. Loss: 0.590 |  Val. Acc: 80.78%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.82%\n",
      "\t Val. Loss: 0.640 |  Val. Acc: 81.27%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.102 | Train Acc: 96.42%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 82.46%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.094 | Train Acc: 96.88%\n",
      "\t Val. Loss: 0.757 |  Val. Acc: 80.51%\n",
      "Test Loss: 0.439 | Test Acc: 81.05%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.37%\n",
      "\t Val. Loss: 0.453 |  Val. Acc: 82.32%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.181 | Train Acc: 93.38%\n",
      "\t Val. Loss: 0.444 |  Val. Acc: 83.19%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.141 | Train Acc: 94.95%\n",
      "\t Val. Loss: 0.585 |  Val. Acc: 80.69%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.117 | Train Acc: 95.88%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 79.22%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.104 | Train Acc: 96.44%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 81.03%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.094 | Train Acc: 96.75%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 81.36%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.083 | Train Acc: 97.24%\n",
      "\t Val. Loss: 0.676 |  Val. Acc: 81.54%\n",
      "Test Loss: 0.410 | Test Acc: 84.03%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.38%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 83.04%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.182 | Train Acc: 93.21%\n",
      "\t Val. Loss: 0.584 |  Val. Acc: 79.60%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.142 | Train Acc: 94.91%\n",
      "\t Val. Loss: 0.552 |  Val. Acc: 81.56%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.81%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 79.64%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.104 | Train Acc: 96.40%\n",
      "\t Val. Loss: 0.637 |  Val. Acc: 81.23%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.092 | Train Acc: 96.89%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 80.96%\n",
      "Test Loss: 0.383 | Test Acc: 83.24%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.39%\n",
      "\t Val. Loss: 0.444 |  Val. Acc: 81.07%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.180 | Train Acc: 93.35%\n",
      "\t Val. Loss: 0.537 |  Val. Acc: 80.87%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.141 | Train Acc: 94.95%\n",
      "\t Val. Loss: 0.575 |  Val. Acc: 81.18%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.120 | Train Acc: 95.86%\n",
      "\t Val. Loss: 0.674 |  Val. Acc: 79.67%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.29%\n",
      "\t Val. Loss: 0.809 |  Val. Acc: 77.95%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.093 | Train Acc: 96.82%\n",
      "\t Val. Loss: 0.624 |  Val. Acc: 81.76%\n",
      "Test Loss: 0.390 | Test Acc: 82.55%\n"
     ]
    }
   ],
   "source": [
    "data_name = \"SST\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device)\n",
    "\n",
    "### Statis Model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "MODE = \"static\"\n",
    "\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_static_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"non-static\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_nonstatic_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))\n",
    "\n",
    "MODE = \"multichannel\"\n",
    "model = KimCNN(MODE, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.static_embed.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.static_embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.static_embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "res_kimcnn = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res_kimcnn[\"num_param\"] = count_parameters(model)\n",
    "with open(\"kimCNN_multichannel_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res_kimcnn, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8d56d-6032-4f02-b321-e6a30e06d04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
