{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1322475c-e25d-4165-bc82-2728addf6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from load_data import load_dataset\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c3a6a-ff63-4e24-a35f-bc5d4570f4b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Common functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a7613e-d1b4-423c-b838-74f1c718616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def multi_models(model, train_iterator, valid_iterator,  test_iterator, num = 5, N_EPOCHS = 20, early_stopping = 5):\n",
    "    init_model = copy.deepcopy(model)\n",
    "    res = {}\n",
    "    res[\"test_loss\"] = []\n",
    "    res[\"test_acc\"] = []\n",
    "    ### begin training process\n",
    "    for i in range(num):\n",
    "        print('Begin training model %s'%i)\n",
    "        res[\"model_%s\"%i] = {}\n",
    "        res[\"model_%s\"%i][\"time\"] = []\n",
    "        res[\"model_%s\"%i][\"train_loss\"] = []\n",
    "        res[\"model_%s\"%i][\"val_loss\"] = []\n",
    "        res[\"model_%s\"%i][\"train_acc\"] = []\n",
    "        res[\"model_%s\"%i][\"val_acc\"] = []\n",
    "        best_valid_loss = float('inf')\n",
    "        num_steps = 0\n",
    "        ### reset model for each round\n",
    "        # model.load_state_dict(init_param)\n",
    "        model = copy.deepcopy(init_model)\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion = criterion.to(device)\n",
    "        \n",
    "        for epoch in range(N_EPOCHS):\n",
    "            print('Begin epoch %s'%epoch)\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "            valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "            end_time = time.time()\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), 'bilstm-model.pt')\n",
    "                num_steps = 0\n",
    "            else:\n",
    "                num_steps += 1\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "            res[\"model_%s\"%i][\"time\"].append(end_time - start_time)\n",
    "            res[\"model_%s\"%i][\"train_loss\"].append(train_loss)\n",
    "            res[\"model_%s\"%i][\"val_loss\"].append(valid_loss)\n",
    "            res[\"model_%s\"%i][\"train_acc\"].append(train_acc)\n",
    "            res[\"model_%s\"%i][\"val_acc\"].append(valid_acc)\n",
    "            if num_steps >= early_stopping:\n",
    "                break\n",
    "        \n",
    "        model.load_state_dict(torch.load('bilstm-model.pt'))\n",
    "        test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        res[\"test_loss\"].append(test_loss)\n",
    "        res[\"test_acc\"].append(test_acc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb67f9-2f8d-40bb-8739-d496b9d526df",
   "metadata": {},
   "source": [
    "### BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b725d1e-bf13-46cf-b713-6977fb6098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175eaa3-fe55-4740-a13f-bb5102a58a46",
   "metadata": {},
   "source": [
    "### train on SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff6510f-3974-4f05-b46b-4c2f85bb2eb3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/jilab/changxin/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.344 | Train Acc: 84.93%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 82.19%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.274 | Train Acc: 88.20%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 83.62%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.235 | Train Acc: 90.31%\n",
      "\t Val. Loss: 0.414 |  Val. Acc: 82.66%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.204 | Train Acc: 91.72%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 84.00%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.178 | Train Acc: 92.92%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 86.27%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.158 | Train Acc: 93.80%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 86.65%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.142 | Train Acc: 94.48%\n",
      "\t Val. Loss: 0.431 |  Val. Acc: 85.94%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.128 | Train Acc: 95.08%\n",
      "\t Val. Loss: 0.423 |  Val. Acc: 86.54%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.120 | Train Acc: 95.37%\n",
      "\t Val. Loss: 0.464 |  Val. Acc: 86.96%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.110 | Train Acc: 95.78%\n",
      "\t Val. Loss: 0.431 |  Val. Acc: 86.00%\n",
      "Test Loss: 0.328 | Test Acc: 86.49%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.343 | Train Acc: 84.92%\n",
      "\t Val. Loss: 0.401 |  Val. Acc: 83.42%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.46%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 84.49%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.240 | Train Acc: 90.11%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 84.24%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.207 | Train Acc: 91.68%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 87.83%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.181 | Train Acc: 92.71%\n",
      "\t Val. Loss: 0.362 |  Val. Acc: 85.16%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.164 | Train Acc: 93.57%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 85.56%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.144 | Train Acc: 94.39%\n",
      "\t Val. Loss: 0.429 |  Val. Acc: 85.13%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.130 | Train Acc: 94.88%\n",
      "\t Val. Loss: 0.426 |  Val. Acc: 84.44%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.122 | Train Acc: 95.33%\n",
      "\t Val. Loss: 0.443 |  Val. Acc: 84.73%\n",
      "Test Loss: 0.339 | Test Acc: 86.37%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.343 | Train Acc: 84.93%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 82.90%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.273 | Train Acc: 88.39%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 84.64%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.233 | Train Acc: 90.31%\n",
      "\t Val. Loss: 0.407 |  Val. Acc: 83.95%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.199 | Train Acc: 91.97%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 83.30%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.174 | Train Acc: 93.17%\n",
      "\t Val. Loss: 0.419 |  Val. Acc: 84.64%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.156 | Train Acc: 93.91%\n",
      "\t Val. Loss: 0.467 |  Val. Acc: 84.29%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.141 | Train Acc: 94.54%\n",
      "\t Val. Loss: 0.401 |  Val. Acc: 85.20%\n",
      "Test Loss: 0.322 | Test Acc: 86.17%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.344 | Train Acc: 84.79%\n",
      "\t Val. Loss: 0.405 |  Val. Acc: 81.27%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.274 | Train Acc: 88.28%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 84.91%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.232 | Train Acc: 90.35%\n",
      "\t Val. Loss: 0.366 |  Val. Acc: 84.98%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.200 | Train Acc: 92.02%\n",
      "\t Val. Loss: 0.365 |  Val. Acc: 85.54%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.175 | Train Acc: 93.02%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 84.08%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.155 | Train Acc: 93.90%\n",
      "\t Val. Loss: 0.441 |  Val. Acc: 84.75%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.140 | Train Acc: 94.61%\n",
      "\t Val. Loss: 0.421 |  Val. Acc: 85.65%\n",
      "Test Loss: 0.338 | Test Acc: 85.42%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.343 | Train Acc: 84.83%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 83.06%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.273 | Train Acc: 88.43%\n",
      "\t Val. Loss: 0.386 |  Val. Acc: 82.97%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.234 | Train Acc: 90.24%\n",
      "\t Val. Loss: 0.382 |  Val. Acc: 83.53%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.202 | Train Acc: 91.78%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 83.57%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.176 | Train Acc: 93.01%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 86.50%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.155 | Train Acc: 93.93%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 85.60%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.140 | Train Acc: 94.56%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 84.58%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.130 | Train Acc: 94.93%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 83.64%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.43%\n",
      "\t Val. Loss: 0.427 |  Val. Acc: 85.76%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.111 | Train Acc: 95.80%\n",
      "\t Val. Loss: 0.436 |  Val. Acc: 87.05%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.03%\n",
      "\t Val. Loss: 0.552 |  Val. Acc: 83.91%\n",
      "Test Loss: 0.321 | Test Acc: 87.48%\n"
     ]
    }
   ],
   "source": [
    "data_name = \"SST\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device, include_lengths = True, batch_first = False)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = biLSTM(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS, \n",
    "                BIDIRECTIONAL, \n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "### NOTE: Probably we need to try fit embedding\n",
    "res = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res[\"num_param\"] = count_parameters(model)\n",
    "with open(\"bilstm_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed303117-4807-4574-849b-38b27aafe8ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.087 | Train Acc: 59.57%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 75.99%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.626 | Train Acc: 77.93%\n",
      "\t Val. Loss: 0.522 |  Val. Acc: 80.98%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.514 | Train Acc: 81.24%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 85.63%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.446 | Train Acc: 84.11%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 83.89%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.374 | Train Acc: 86.91%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 87.36%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.325 | Train Acc: 88.09%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 87.72%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.290 | Train Acc: 89.10%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 85.64%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.258 | Train Acc: 90.06%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 89.08%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.205 | Train Acc: 92.50%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 88.58%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.187 | Train Acc: 93.38%\n",
      "\t Val. Loss: 0.384 |  Val. Acc: 87.53%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.43%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 88.92%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.91%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 87.73%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.114 | Train Acc: 96.07%\n",
      "\t Val. Loss: 0.337 |  Val. Acc: 88.56%\n",
      "Test Loss: 0.238 | Test Acc: 92.16%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.081 | Train Acc: 59.37%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 79.42%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.621 | Train Acc: 77.43%\n",
      "\t Val. Loss: 0.504 |  Val. Acc: 80.96%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.500 | Train Acc: 81.83%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 85.80%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.426 | Train Acc: 84.15%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 87.01%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.362 | Train Acc: 86.78%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 86.33%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.322 | Train Acc: 88.11%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 86.34%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.290 | Train Acc: 89.56%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 85.62%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.244 | Train Acc: 90.96%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 87.89%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.77%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 87.54%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.176 | Train Acc: 94.03%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 88.57%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.167 | Train Acc: 94.10%\n",
      "\t Val. Loss: 0.315 |  Val. Acc: 87.90%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.146 | Train Acc: 94.90%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 88.91%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.129 | Train Acc: 95.43%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 88.76%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.67%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 88.07%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.37%\n",
      "\t Val. Loss: 0.338 |  Val. Acc: 89.60%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.087 | Train Acc: 96.91%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 89.79%\n",
      "Test Loss: 0.256 | Test Acc: 89.90%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.070 | Train Acc: 60.28%\n",
      "\t Val. Loss: 0.608 |  Val. Acc: 76.82%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.633 | Train Acc: 76.45%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 83.20%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.514 | Train Acc: 80.78%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 85.80%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.424 | Train Acc: 84.45%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 87.52%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.379 | Train Acc: 85.93%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 88.39%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.341 | Train Acc: 87.43%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 87.71%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.285 | Train Acc: 89.62%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 87.71%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.260 | Train Acc: 90.25%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 89.44%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.213 | Train Acc: 92.18%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 87.90%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.177 | Train Acc: 93.68%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 89.60%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.166 | Train Acc: 93.85%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 89.60%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.151 | Train Acc: 94.69%\n",
      "\t Val. Loss: 0.361 |  Val. Acc: 89.26%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.127 | Train Acc: 96.06%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 88.92%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.108 | Train Acc: 96.30%\n",
      "\t Val. Loss: 0.354 |  Val. Acc: 89.27%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.093 | Train Acc: 96.74%\n",
      "\t Val. Loss: 0.314 |  Val. Acc: 90.82%\n",
      "Test Loss: 0.233 | Test Acc: 92.74%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.083 | Train Acc: 58.51%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 78.55%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.619 | Train Acc: 76.91%\n",
      "\t Val. Loss: 0.459 |  Val. Acc: 82.34%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.484 | Train Acc: 82.35%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 85.44%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.414 | Train Acc: 85.14%\n",
      "\t Val. Loss: 0.351 |  Val. Acc: 87.53%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.366 | Train Acc: 86.41%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 86.50%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.324 | Train Acc: 87.96%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 86.15%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.284 | Train Acc: 89.79%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 88.06%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.243 | Train Acc: 90.92%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 87.53%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.217 | Train Acc: 91.91%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 89.08%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.195 | Train Acc: 92.89%\n",
      "\t Val. Loss: 0.325 |  Val. Acc: 87.89%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.157 | Train Acc: 94.33%\n",
      "\t Val. Loss: 0.343 |  Val. Acc: 88.41%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.131 | Train Acc: 95.37%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 89.09%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.122 | Train Acc: 95.80%\n",
      "\t Val. Loss: 0.311 |  Val. Acc: 89.96%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.111 | Train Acc: 96.23%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 87.22%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.84%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 88.39%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.090 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.375 |  Val. Acc: 87.88%\n",
      "Begin epoch 16\n",
      "\tTrain Loss: 0.077 | Train Acc: 96.91%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 89.43%\n",
      "Begin epoch 17\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 89.27%\n",
      "Test Loss: 0.242 | Test Acc: 93.37%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 1.077 | Train Acc: 60.01%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 76.48%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.627 | Train Acc: 76.93%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 82.03%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.502 | Train Acc: 81.59%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 86.84%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.436 | Train Acc: 83.94%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 84.42%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.377 | Train Acc: 85.92%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 86.65%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.324 | Train Acc: 88.06%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 87.53%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.281 | Train Acc: 89.80%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 88.56%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.244 | Train Acc: 91.20%\n",
      "\t Val. Loss: 0.304 |  Val. Acc: 89.09%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.210 | Train Acc: 92.27%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 89.10%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.184 | Train Acc: 93.20%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 89.09%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.152 | Train Acc: 94.67%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 89.09%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.138 | Train Acc: 95.22%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 89.96%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.131 | Train Acc: 95.59%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 88.41%\n",
      "Test Loss: 0.298 | Test Acc: 89.81%\n"
     ]
    }
   ],
   "source": [
    "data_name = \"TREC\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device, include_lengths = True, batch_first = False)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = biLSTM(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS, \n",
    "                BIDIRECTIONAL, \n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "### NOTE: Probably we need to try fit embedding\n",
    "res = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res[\"num_param\"] = count_parameters(model)\n",
    "with open(\"bilstm_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9beb9a62-30af-43c4-be49-89fde313c569",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/jilab/changxin/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training model 0\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.607 | Train Acc: 67.11%\n",
      "\t Val. Loss: 0.494 |  Val. Acc: 75.82%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.528 | Train Acc: 73.10%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 84.80%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.358 | Train Acc: 84.51%\n",
      "\t Val. Loss: 0.307 |  Val. Acc: 87.50%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.313 | Train Acc: 86.99%\n",
      "\t Val. Loss: 0.283 |  Val. Acc: 87.97%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.291 | Train Acc: 87.76%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 88.67%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.271 | Train Acc: 88.85%\n",
      "\t Val. Loss: 0.248 |  Val. Acc: 89.84%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.254 | Train Acc: 89.72%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 89.77%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.241 | Train Acc: 90.15%\n",
      "\t Val. Loss: 0.242 |  Val. Acc: 90.08%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.225 | Train Acc: 90.85%\n",
      "\t Val. Loss: 0.238 |  Val. Acc: 89.96%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.207 | Train Acc: 91.55%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 90.31%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.190 | Train Acc: 92.39%\n",
      "\t Val. Loss: 0.228 |  Val. Acc: 90.51%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.176 | Train Acc: 92.91%\n",
      "\t Val. Loss: 0.232 |  Val. Acc: 90.90%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.164 | Train Acc: 93.56%\n",
      "\t Val. Loss: 0.239 |  Val. Acc: 90.86%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.152 | Train Acc: 94.03%\n",
      "\t Val. Loss: 0.242 |  Val. Acc: 90.86%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.138 | Train Acc: 94.66%\n",
      "\t Val. Loss: 0.254 |  Val. Acc: 91.09%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.120 | Train Acc: 95.35%\n",
      "\t Val. Loss: 0.282 |  Val. Acc: 90.74%\n",
      "Test Loss: 0.229 | Test Acc: 90.79%\n",
      "Begin training model 1\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.591 | Train Acc: 68.32%\n",
      "\t Val. Loss: 0.443 |  Val. Acc: 80.94%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.525 | Train Acc: 73.66%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 65.59%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.400 | Train Acc: 82.77%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 85.23%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.341 | Train Acc: 85.98%\n",
      "\t Val. Loss: 0.306 |  Val. Acc: 87.70%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.308 | Train Acc: 87.41%\n",
      "\t Val. Loss: 0.280 |  Val. Acc: 88.52%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.292 | Train Acc: 88.16%\n",
      "\t Val. Loss: 0.273 |  Val. Acc: 89.10%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.269 | Train Acc: 89.16%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 88.75%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.259 | Train Acc: 89.63%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 90.27%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.237 | Train Acc: 90.54%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 88.63%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.227 | Train Acc: 90.83%\n",
      "\t Val. Loss: 0.234 |  Val. Acc: 90.62%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.215 | Train Acc: 91.29%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 90.47%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.201 | Train Acc: 91.95%\n",
      "\t Val. Loss: 0.230 |  Val. Acc: 90.82%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.184 | Train Acc: 92.65%\n",
      "\t Val. Loss: 0.224 |  Val. Acc: 90.82%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.174 | Train Acc: 93.23%\n",
      "\t Val. Loss: 0.232 |  Val. Acc: 91.17%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.164 | Train Acc: 93.53%\n",
      "\t Val. Loss: 0.231 |  Val. Acc: 90.90%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.18%\n",
      "\t Val. Loss: 0.260 |  Val. Acc: 90.70%\n",
      "Begin epoch 16\n",
      "\tTrain Loss: 0.136 | Train Acc: 94.58%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 91.56%\n",
      "Begin epoch 17\n",
      "\tTrain Loss: 0.125 | Train Acc: 95.06%\n",
      "\t Val. Loss: 0.249 |  Val. Acc: 90.78%\n",
      "Test Loss: 0.232 | Test Acc: 91.04%\n",
      "Begin training model 2\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.631 | Train Acc: 64.73%\n",
      "\t Val. Loss: 0.531 |  Val. Acc: 72.77%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.549 | Train Acc: 71.52%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 85.23%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.359 | Train Acc: 84.85%\n",
      "\t Val. Loss: 0.318 |  Val. Acc: 86.76%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.322 | Train Acc: 86.64%\n",
      "\t Val. Loss: 0.304 |  Val. Acc: 88.28%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.303 | Train Acc: 87.29%\n",
      "\t Val. Loss: 0.286 |  Val. Acc: 89.02%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.37%\n",
      "\t Val. Loss: 0.271 |  Val. Acc: 88.75%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.266 | Train Acc: 89.11%\n",
      "\t Val. Loss: 0.254 |  Val. Acc: 89.88%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.255 | Train Acc: 89.57%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 89.80%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.245 | Train Acc: 90.12%\n",
      "\t Val. Loss: 0.260 |  Val. Acc: 88.71%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.235 | Train Acc: 90.64%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 90.55%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.17%\n",
      "\t Val. Loss: 0.254 |  Val. Acc: 89.88%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.201 | Train Acc: 92.08%\n",
      "\t Val. Loss: 0.259 |  Val. Acc: 90.04%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.196 | Train Acc: 92.12%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 90.20%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.176 | Train Acc: 93.18%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 90.00%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.165 | Train Acc: 93.66%\n",
      "\t Val. Loss: 0.283 |  Val. Acc: 90.35%\n",
      "Test Loss: 0.242 | Test Acc: 90.31%\n",
      "Begin training model 3\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.583 | Train Acc: 68.92%\n",
      "\t Val. Loss: 0.471 |  Val. Acc: 80.31%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.487 | Train Acc: 76.62%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 83.71%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.446 | Train Acc: 79.66%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 80.94%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.342 | Train Acc: 85.42%\n",
      "\t Val. Loss: 0.276 |  Val. Acc: 87.97%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.299 | Train Acc: 87.65%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 88.75%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.273 | Train Acc: 88.77%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 89.06%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.259 | Train Acc: 89.42%\n",
      "\t Val. Loss: 0.237 |  Val. Acc: 89.92%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.244 | Train Acc: 90.01%\n",
      "\t Val. Loss: 0.230 |  Val. Acc: 90.66%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.229 | Train Acc: 90.74%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 89.69%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.212 | Train Acc: 91.38%\n",
      "\t Val. Loss: 0.234 |  Val. Acc: 90.39%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.197 | Train Acc: 92.28%\n",
      "\t Val. Loss: 0.246 |  Val. Acc: 89.96%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.183 | Train Acc: 92.67%\n",
      "\t Val. Loss: 0.226 |  Val. Acc: 91.29%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.171 | Train Acc: 93.06%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 90.47%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.152 | Train Acc: 94.05%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 90.98%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.144 | Train Acc: 94.30%\n",
      "\t Val. Loss: 0.237 |  Val. Acc: 90.59%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.127 | Train Acc: 95.14%\n",
      "\t Val. Loss: 0.256 |  Val. Acc: 91.05%\n",
      "Begin epoch 16\n",
      "\tTrain Loss: 0.117 | Train Acc: 95.42%\n",
      "\t Val. Loss: 0.261 |  Val. Acc: 90.35%\n",
      "Test Loss: 0.234 | Test Acc: 90.69%\n",
      "Begin training model 4\n",
      "Begin epoch 0\n",
      "\tTrain Loss: 0.608 | Train Acc: 66.67%\n",
      "\t Val. Loss: 0.476 |  Val. Acc: 78.87%\n",
      "Begin epoch 1\n",
      "\tTrain Loss: 0.496 | Train Acc: 75.72%\n",
      "\t Val. Loss: 0.433 |  Val. Acc: 79.80%\n",
      "Begin epoch 2\n",
      "\tTrain Loss: 0.529 | Train Acc: 73.47%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 81.25%\n",
      "Begin epoch 3\n",
      "\tTrain Loss: 0.347 | Train Acc: 85.03%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 86.56%\n",
      "Begin epoch 4\n",
      "\tTrain Loss: 0.305 | Train Acc: 87.41%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 88.24%\n",
      "Begin epoch 5\n",
      "\tTrain Loss: 0.283 | Train Acc: 88.07%\n",
      "\t Val. Loss: 0.268 |  Val. Acc: 89.30%\n",
      "Begin epoch 6\n",
      "\tTrain Loss: 0.270 | Train Acc: 88.95%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 89.18%\n",
      "Begin epoch 7\n",
      "\tTrain Loss: 0.258 | Train Acc: 89.20%\n",
      "\t Val. Loss: 0.248 |  Val. Acc: 89.18%\n",
      "Begin epoch 8\n",
      "\tTrain Loss: 0.245 | Train Acc: 90.07%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 90.23%\n",
      "Begin epoch 9\n",
      "\tTrain Loss: 0.233 | Train Acc: 90.48%\n",
      "\t Val. Loss: 0.268 |  Val. Acc: 89.18%\n",
      "Begin epoch 10\n",
      "\tTrain Loss: 0.218 | Train Acc: 91.17%\n",
      "\t Val. Loss: 0.237 |  Val. Acc: 90.31%\n",
      "Begin epoch 11\n",
      "\tTrain Loss: 0.210 | Train Acc: 91.54%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 90.43%\n",
      "Begin epoch 12\n",
      "\tTrain Loss: 0.195 | Train Acc: 92.23%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 90.20%\n",
      "Begin epoch 13\n",
      "\tTrain Loss: 0.177 | Train Acc: 92.79%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 90.00%\n",
      "Begin epoch 14\n",
      "\tTrain Loss: 0.176 | Train Acc: 92.83%\n",
      "\t Val. Loss: 0.241 |  Val. Acc: 90.62%\n",
      "Begin epoch 15\n",
      "\tTrain Loss: 0.150 | Train Acc: 94.05%\n",
      "\t Val. Loss: 0.247 |  Val. Acc: 90.16%\n",
      "Begin epoch 16\n",
      "\tTrain Loss: 0.143 | Train Acc: 94.39%\n",
      "\t Val. Loss: 0.261 |  Val. Acc: 90.31%\n",
      "Test Loss: 0.228 | Test Acc: 90.85%\n"
     ]
    }
   ],
   "source": [
    "data_name = \"IMDB\"\n",
    "train_iterator, valid_iterator, test_iterator, TEXT, LABEL = load_dataset(data_name, device, include_lengths = True, batch_first = False)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = biLSTM(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS, \n",
    "                BIDIRECTIONAL, \n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "\n",
    "### pre-train embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "### NOTE: Probably we need to try fit embedding\n",
    "res = multi_models(model, train_iterator, valid_iterator, test_iterator)\n",
    "res[\"num_param\"] = count_parameters(model)\n",
    "with open(\"bilstm_%s.json\"%data_name, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f262441-536f-4f2a-bb30-e45f3968bc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc99c37-d9b3-4b4c-901e-d3bf5d55532f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5a48e-5bbd-4483-9949-db73765056ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
